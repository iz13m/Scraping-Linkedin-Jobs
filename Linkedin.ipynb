{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries..\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating chrome web driver..\n",
    "browser = webdriver.Chrome('/Users/iz13m/Code/Linkedin/chromedriver')\n",
    "browser.set_window_position(900, 0)\n",
    "browser.set_window_size(500, 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open login page\n",
    "browser.get('https://www.linkedin.com/login?fromSignIn=true&trk=guest_homepage-basic_nav-header-signin')\n",
    "\n",
    "#Enter login info:\n",
    "elementID = browser.find_element_by_id('username')\n",
    "elementID.send_keys('*********@gmail.com')\n",
    "\n",
    "elementID = browser.find_element_by_id('password')\n",
    "elementID.send_keys('**********')\n",
    "elementID.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Job search page\n",
    "URL = 'https://www.linkedin.com/jobs/search/?geoId=106155005&location=Egypt&sortBy=DD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paging(url):\n",
    "    # This function returns pages urls\n",
    "    # It's defaulted to return 199 pages.. you can edit the number\n",
    "    browser.get(url)\n",
    "    time.sleep(1)\n",
    "    lst = []\n",
    "    for i in range(200):\n",
    "        link = url +\"&start={}\".format(7*i+1)\n",
    "        lst.append(link)\n",
    "    print('Pages Created Successfully .. ')\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jobs(url):\n",
    "    # This function return jobs urls in given jobs search page.. \n",
    "    browser.get(url)\n",
    "    time.sleep(2)\n",
    "    lst = []\n",
    "    src = browser.page_source\n",
    "    soup = BeautifulSoup(src, 'lxml')\n",
    "    job_containers = soup.find_all('div', class_ = 'full-width artdeco-entity-lockup__title ember-view')\n",
    "    link = str(job_containers[0])\n",
    "    jobs=[]\n",
    "    try:\n",
    "        for i in range(len(job_containers)):\n",
    "            link = str(job_containers[i])\n",
    "            try:\n",
    "                jobid = re.findall(r\"jobs/view/\\d\\d\\d\\d\\d\\d\\d\\d\\d\\d\",link)[0].split('/')[-1]\n",
    "                jobs.append(\"https://www.linkedin.com/jobs/view/\" + jobid)\n",
    "                print(i,' success')\n",
    "            except:\n",
    "                print(i, \"skipped\")\n",
    "    except:\n",
    "        print(\"loop skipped..\")\n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This part starts the process of creating pages url and then adding them to a list\n",
    "pages = paging(URL)\n",
    "all_links = []\n",
    "for page in pages:\n",
    "    links = get_jobs(page)\n",
    "    all_links.extend(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for the count of urls\n",
    "len(all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapper(url):\n",
    "    # This function is responsible for scrapping job information of a given job url\n",
    "    browser.get(url)\n",
    "    time.sleep(2)\n",
    "    src = browser.page_source\n",
    "    soup = BeautifulSoup(src, 'lxml')\n",
    "    values={}\n",
    "    \n",
    "    job_title = soup.find('h1', class_ = 't-24 t-bold').text\n",
    "    try:\n",
    "        company_name = soup.find('span', class_ = 'jobs-unified-top-card__subtitle-primary-grouping mr2 t-black').span.text.strip()\n",
    "    except:\n",
    "        company_name=\"\"\n",
    "    try:\n",
    "        location = soup.find('span', class_ = 'jobs-unified-top-card__bullet').text.strip()\n",
    "    except:\n",
    "        location=\"\"\n",
    "    try:\n",
    "        date = soup.find('span', class_ = 'jobs-unified-top-card__posted-date').text.strip()\n",
    "    except:\n",
    "        date=\"\"\n",
    "    try:\n",
    "        job_bar = soup.find(attrs={\"type\":\"briefcase-icon\"}).parent.parent.parent.next_sibling.next_sibling.text\n",
    "    except:\n",
    "        job_bar=\"\"\n",
    "    try:\n",
    "        com_bar = soup.find(attrs={\"type\":\"company-icon\"}).parent.parent.parent.next_sibling.next_sibling.text\n",
    "    except:\n",
    "        com_bar = \"\"\n",
    "    try:\n",
    "        job_type = job_bar.split(\"路\")[0]\n",
    "    except:\n",
    "        job_type = \"\"\n",
    "    try:\n",
    "        job_level = job_bar.split(\"路\")[1]\n",
    "    except:\n",
    "        job_level=\"\"\n",
    "    try:\n",
    "        emp_count = com_bar.split(\"路\")[0] \n",
    "    except:\n",
    "        emp_count=\"\"\n",
    "    try:\n",
    "        industry = com_bar.split(\"路\")[1]\n",
    "    except:\n",
    "        industry = \"\"\n",
    "    try:\n",
    "        salary = soup.find('h2', class_= 't-16 pt4 ph5').text.strip()\n",
    "    except:\n",
    "        salary=\"\"\n",
    "        \n",
    "    description = \" \".join([text for text in soup.find(id=\"job-details\").stripped_strings])\n",
    "        \n",
    "    values = {\"Title\":job_title,\"Company Name\": company_name, \"Location\":location, \"Date\":date,\n",
    "              \"Job Type\":job_type, \"Seneiority Level\":job_level, \"Company Size\": emp_count,\n",
    "             \"Industry\": industry, \"About The Job\": description, \"Pay Range\": salary, \"Job URL\":url}\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This part feeds the scrapper function jobs urls from a staring index\n",
    "# Starting index is used as the code may break during scrapping ..\n",
    "# .. due to server errors from linkedin or any other internet related problems ..\n",
    "# .. which enables us to continue from where we stopped.\n",
    "df = []\n",
    "start_pos = 369\n",
    "for i in range(start_pos,len(all_links)):\n",
    "    dic = scrapper(all_links[i])\n",
    "    df.append(dic)\n",
    "    print(i,\" Success\")\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for the count of jobs..\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting data into pandas dataframe ..\n",
    "df_ = pd.DataFrame(df)\n",
    "df_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting data into a CSV file\n",
    "df_.to_csv(\"Linkedin.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
